{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPUKuKUTRAAQRBCVAQWH3N/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junafndiku/bachelor-thesis-vgg-alexnet/blob/main/AlexNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d abdelghaniaaba/wildfire-prediction-dataset\n",
        "! unzip /content/wildfire-prediction-dataset.zip"
      ],
      "metadata": {
        "id": "GUdSV5FMrD44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.layers import BatchNormalization"
      ],
      "metadata": {
        "id": "wCGFxizSUC5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_width, img_height = 227, 227\n",
        "train_data_dir = 'train'\n",
        "validation_data_dir = 'test'\n",
        "epochs = 10\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "_J9m_nSuUFFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if K.image_data_format() == 'channels_first':\n",
        "    input_shape = (3, img_width, img_height)\n",
        "else:\n",
        "    input_shape = (img_width, img_height, 3)\n",
        "\n",
        "train_datagen = ImageDataGenerator()\n",
        "\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n"
      ],
      "metadata": {
        "id": "YAykQW3RrG6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "model.add(Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), padding=\"valid\", activation=\"relu\", input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), padding=\"valid\", activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\", activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Flatten layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Dense(4096, activation=\"relu\"))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(4096, activation=\"relu\"))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output layer for binary classification\n",
        "model.add(Dense(2, activation=\"softmax\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "TlYOGt7JrKL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "MjDyQXVdrNgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "hist=model.fit_generator(train_generator, steps_per_epoch = 100,epochs=8, validation_data = validation_generator, validation_steps = 10)"
      ],
      "metadata": {
        "id": "D4Sk_HF7UNuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist.history[\"accuracy\"])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title(\"model accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X7S-hd_AUQya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "szfnNTcJUaMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model_save_name = 'classifier.alexnet'\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\"\n",
        "model.save(path)"
      ],
      "metadata": {
        "id": "NIhn6u-EUhA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.models import load_model\n",
        "model = load_model(\"/content/gdrive/MyDrive/classifier.alexnet\")\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "validation_datagen = ImageDataGenerator()\n",
        "validation_generator = trdata.flow_from_directory(directory=\"/content/valid\",target_size=(227,227))\n",
        "\n",
        "loss, accuracy = model.evaluate(validation_generator, steps=100)\n",
        "print(f'Validation Loss: {loss}')\n",
        "print(f'Validation Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "uoFdscTB1Piw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}